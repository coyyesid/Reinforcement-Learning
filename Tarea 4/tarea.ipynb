{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce9306c",
   "metadata": {},
   "source": [
    "# Tarea 4\n",
    "Camilo Enrique Barreto Reyes\n",
    "Brayan Yesid Coy Palacios\n",
    "\n",
    "En este taller usted debe entrenar un agente para que aprenda a jugar el juego de Atari Boxing:\n",
    "\n",
    "*  Entrene el agente para los niveles de dificultad 1 y 3, con por lo menos dos algoritmos\n",
    " basados en función de valor y dos algoritmos de política de gradiente. Puede utilizar\n",
    " las implementaciones en la librería Stable Baselines3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0543e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d6823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Dificultad 1 ====\n",
      "\n",
      "Entrenando DQN en dificultad 1...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Usando buffer_size reducido a 50000 para DQN\n",
      "DQN dificultad 1: recompensa media=-31.0, std=0.0, tiempo=28.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coyye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d1\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d1\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d1\\rl-video-episode-0.mp4\n",
      "Video guardado en ./videos_DQN_d1\n",
      "\n",
      "Entrenando PPO en dificultad 1...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 228  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "PPO dificultad 1: recompensa media=-10.4, std=8.8, tiempo=47.8s\n",
      "MoviePy - Building video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_PPO_d1\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_PPO_d1\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_PPO_d1\\rl-video-episode-0.mp4\n",
      "Video guardado en ./videos_PPO_d1\n",
      "\n",
      "Entrenando A2C en dificultad 1...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 66       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.88    |\n",
      "|    explained_variance | 0.0905   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0561  |\n",
      "|    value_loss         | 0.000438 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.89     |\n",
      "|    explained_variance | -59.3     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.000881 |\n",
      "|    value_loss         | 2.13e-07  |\n",
      "-------------------------------------\n",
      "A2C dificultad 1: recompensa media=-43.8, std=4.4, tiempo=15.0s\n",
      "MoviePy - Building video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_A2C_d1\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_A2C_d1\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_A2C_d1\\rl-video-episode-0.mp4\n",
      "Video guardado en ./videos_A2C_d1\n",
      "\n",
      "==== Dificultad 3 ====\n",
      "\n",
      "Entrenando DQN en dificultad 3...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Usando buffer_size reducido a 50000 para DQN\n",
      "DQN dificultad 3: recompensa media=-2.6, std=1.3564659966250538, tiempo=71.1s\n",
      "MoviePy - Building video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d3\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d3\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_DQN_d3\\rl-video-episode-0.mp4\n",
      "Video guardado en ./videos_DQN_d3\n",
      "\n",
      "Entrenando PPO en dificultad 3...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 229  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "PPO dificultad 3: recompensa media=-1.0, std=2.6076809620810595, tiempo=47.7s\n",
      "MoviePy - Building video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_PPO_d3\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_PPO_d3\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_PPO_d3\\rl-video-episode-0.mp4\n",
      "Video guardado en ./videos_PPO_d3\n",
      "\n",
      "Entrenando A2C en dificultad 3...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 67       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.89    |\n",
      "|    explained_variance | -3.69    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00293  |\n",
      "|    value_loss         | 1.43e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.89     |\n",
      "|    explained_variance | -3.36e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.00588   |\n",
      "|    value_loss         | 7.86e-06  |\n",
      "-------------------------------------\n",
      "A2C dificultad 3: recompensa media=-3.0, std=0.0, tiempo=15.1s\n",
      "MoviePy - Building video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_A2C_d3\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_A2C_d3\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\coyye\\OneDrive - Universidad de los andes\\DOCTORADO\\Primer Semestre\\Reinforcement Learning\\Tarea 4\\videos\\videos_A2C_d3\\rl-video-episode-0.mp4\n",
      "Video guardado en ./videos_A2C_d3\n",
      "\n",
      "==== Comparación de resultados ====\n",
      "  Algoritmo  Dificultad  Recompensa Media  Desviación  \\\n",
      "0       DQN           1             -31.0    0.000000   \n",
      "1       PPO           1             -10.4    8.800000   \n",
      "2       A2C           1             -43.8    4.400000   \n",
      "3       DQN           3              -2.6    1.356466   \n",
      "4       PPO           3              -1.0    2.607681   \n",
      "5       A2C           3              -3.0    0.000000   \n",
      "\n",
      "   Tiempo Entrenamiento (s)  \n",
      "0                 28.411217  \n",
      "1                 47.772192  \n",
      "2                 15.022671  \n",
      "3                 71.123264  \n",
      "4                 47.695773  \n",
      "5                 15.131379  \n",
      "Tabla guardada en resultados_comparacion.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dificultad = [1, 3]\n",
    "algortimo = {\n",
    "    'DQN': DQN,\n",
    "    'PPO': PPO,\n",
    "    'A2C': A2C\n",
    "}\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for diff in dificultad:\n",
    "    print(f\"\\n==== Dificultad {diff} ====\")\n",
    "    ambiente = \"ALE/Boxing-v5\"\n",
    "    ambien = DummyVecEnv([lambda: gym.make(ambiente, difficulty=diff)])\n",
    "\n",
    "    for nombre_algoritmo, clase_algortimos in algortimo.items():\n",
    "        print(f\"\\n{nombre_algoritmo} en dificultad {diff}...\")\n",
    "        politica = \"CnnPolicy\"\n",
    "\n",
    "        if nombre_algoritmo == \"DQN\":\n",
    "            modelo = clase_algortimos(politica, ambien, verbose=1, buffer_size=5000)\n",
    "            print(f\"Usando buffer_size reducido a 50000 para DQN\")\n",
    "        else:\n",
    "            modelo = clase_algortimos(politica, ambien, verbose=1)\n",
    "\n",
    "        tiempo_inicial = time.time()\n",
    "        modelo.learn(total_timesteps=1000)\n",
    "        diracion = time.time() - tiempo_inicial\n",
    "        modelo.save(f\"{nombre_algoritmo}_boxing_d{diff}\")\n",
    "\n",
    "        promedio_recompe, desviasion = evaluate_policy(modelo, ambien, n_eval_episodes=5)\n",
    "        print(f\"{nombre_algoritmo} dificultad {diff}: recompensa media={promedio_recompe}, std={desviasion}, tiempo={diracion:.1f}s\")\n",
    "\n",
    "        resultados.append({\n",
    "            'Algoritmo': nombre_algoritmo,\n",
    "            'Dificultad': diff,\n",
    "            'Recompensa Media': promedio_recompe,\n",
    "            'Desviación': desviasion,\n",
    "            'Tiempo Entrenamiento (s)': diracion\n",
    "        })\n",
    "\n",
    "        video_env = RecordVideo(gym.make(ambiente, difficulty=diff, render_mode=\"rgb_array\"),\n",
    "                                video_folder=f\"./videos_{nombre_algoritmo}_d{diff}\", episode_trigger=lambda x: x == 0)\n",
    "        obs, _ = video_env.reset()\n",
    "        for _ in range(3000):\n",
    "            action, _states = modelo.predict(obs)\n",
    "            obs, reward, terminated, truncated, info = video_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                obs, _ = video_env.reset()\n",
    "        video_env.close()\n",
    "        print(f\"Video guardado en ./videos_{nombre_algoritmo}_d{diff}\")\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "print(\"\\n==== Comparación de resultados ====\")\n",
    "print(df_resultados)\n",
    "df_resultados.to_csv(\"resultados_comparacion.csv\", index=False)\n",
    "print(\"Tabla guardada en resultados_comparacion.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
